{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearching for the optimal hyerparameters\n",
    "\n",
    "In this notebook, I used scikit-learn library GridsearchCV() method to find th optimal parameters. \n",
    "The hyperparameters include the number of convolutional layers, the number of filters, the pooling methods, the regularizer methods and alpha value, the number of dense layers and the number of nodes in each layer, and batch size. An exhausting computational power would be required to run all the hyperparameters in 1 gridsearch. That's why I ran more than 20 gridsearches and seeked for the optimal combinations. My decision criteria are 'time', 'accuracy', 'computational power', and 'loss'. Based on these criteria I found that 4 convolutional layers  and 2 dense layers with dropout regularizer is the optimal combination of hyperparameters.\n",
    "\n",
    "In the below cells, you can see the full codes for some of the gridsearches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the library packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.image as img\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are finding the best parameters via gridsearch, used different photos which were obtained from shopping websites, such as macys.com and shop.nordstorm.com. These photos were saved in `train` and `test` folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell I am creating the training data. (X_train and y_train)\n",
    "\n",
    "# A target list that will contain target values\n",
    "target_list = []\n",
    "\n",
    "# A data list that will contain the image data. We will convert it to numpy array\n",
    "data_list = []\n",
    "\n",
    "# There are 44 photos in train folder. I want to randomly select them\n",
    "ph_numbers = np.random.choice(np.linspace(1,44,44), 44, replace=False)\n",
    "\n",
    "# In order to make sure all photos are in the same size, I defined the dimensions.\n",
    "width = int(780)\n",
    "height = int(1196)\n",
    "dimensions = (width, height)\n",
    "\n",
    "for i in ph_numbers:\n",
    "    \n",
    "    # reading each photo from the file. Also converting to float data type\n",
    "    string = img.imread(\"train/\"+str(int(i))+\".jpeg\").astype('float32')\n",
    "    \n",
    "    # squeshing the photo. (normalizing)\n",
    "    string /= 255\n",
    "    \n",
    "    #resizing the photo using cv2\n",
    "    string = cv2.resize(string, dimensions, interpolation = cv2.INTER_AREA) \n",
    "    \n",
    "    # appending in a list where we keep all the photo data\n",
    "    data_list.append(string)\n",
    "    \n",
    "    # if the photo names are in the below list, we know that they are \"not liked\", so we add 0 for them\n",
    "    if i in [16, 3, 24, 37, 10, 26, 38, 11, 27, 39, 29, 40, 14, 41, 18, 34, 42, 22, 23, 36, 43 ,44]:\n",
    "        target_list.append(0)\n",
    "    # if the photo names are not in the above list, they are liked.\n",
    "    else:\n",
    "        target_list.append(1)\n",
    "        \n",
    "\n",
    "# converting the target list into numpy array\n",
    "target_list = np.array(target_list)\n",
    "\n",
    "# converting the image data list to a numpy array\n",
    "X_train = np.array(data_list)\n",
    "\n",
    "y_train = target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell I amcreating the testing data. (X_test and y_test)\n",
    "\n",
    "#loading the test photos, and converting them to float. Also normalizing by diviving by 255\n",
    "t1 = img.imread('test/t7.jpeg').astype('float32') / 255\n",
    "t2 = img.imread('test/t8.jpeg').astype('float32') / 255\n",
    "t3 = img.imread('test/t9.jpeg').astype('float32') / 255\n",
    "t4 = img.imread('test/t10.jpeg').astype('float32') / 255\n",
    "t5 = img.imread('test/t11.jpeg').astype('float32') / 255\n",
    "t6 = img.imread('test/t12.jpeg').astype('float32') / 255\n",
    "\n",
    "#resizing the photos using OpenCV .cv2 function\n",
    "t1 = cv2.resize(t1, dimensions, interpolation = cv2.INTER_AREA) \n",
    "t2 = cv2.resize(t2, dimensions, interpolation = cv2.INTER_AREA) \n",
    "t3 = cv2.resize(t3, dimensions, interpolation = cv2.INTER_AREA) \n",
    "t4 = cv2.resize(t4, dimensions, interpolation = cv2.INTER_AREA) \n",
    "t5 = cv2.resize(t5, dimensions, interpolation = cv2.INTER_AREA) \n",
    "t6 = cv2.resize(t6, dimensions, interpolation = cv2.INTER_AREA) \n",
    "\n",
    "\n",
    "# creating a numpy array of X variables\n",
    "X_test = np.array([t1 , t2 , t3 , t4 , t5 , t6])\n",
    "\n",
    "# creating target values, based on the numpy array sequence\n",
    "y_test = np.array([0,1,1,1,0,0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define a function for the model. I will pass this into the gridsearch\n",
    "\n",
    "def model_fn(layer_one_neurons=32, layer_two_neurons=32, layer_one_dropout=0.3, layer_two_dropout=0.3, \n",
    "             layer_three_neurons=32, layer_three_dropout=0.5):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a convolutional layer.\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(1196, 780, 3)) )\n",
    "    \n",
    "    # add a pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Add a convolutional layer.\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(1196, 780, 3)) )\n",
    "    \n",
    "    # add a pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(layer_one_neurons, activation='relu'))\n",
    "    model.add(Dropout(layer_one_dropout))\n",
    "    \n",
    "    model.add(Dense(layer_two_neurons, activation='relu'))\n",
    "    model.add(Dropout(layer_two_dropout))\n",
    "    \n",
    "    model.add(Dense(layer_three_neurons, activation='relu'))\n",
    "    model.add(Dropout(layer_three_dropout))\n",
    "    \n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "nn = KerasRegressor(build_fn=model_fn, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The params grid:\n",
    "nm_params = {\n",
    "            'layer_one_neurons':[8, 16, 32, 64],\n",
    "             'layer_two_neurons':[8, 16, 32, 64],\n",
    "             'layer_one_dropout':[0.3, 0.4, 0.5],\n",
    "            'layer_two_dropout':[0.3, 0.4, 0.5],\n",
    "            'epochs':[10, 15, 20],\n",
    "            'layer_three_neurons':[8, 16, 32, 64],\n",
    "            'layer_two_dropout':[0.3, 0.4, 0.5]\n",
    "            }\n",
    "\n",
    "gs = GridSearchCV(estimator=nn, param_grid=nm_params, cv=2) # Use cv=3 just for the sake of time! \n",
    "gs.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the required computational power is too high, I created few more gridsearch to try out different hyperparamaters. Especially for different reularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for a model with LASSO regularizer\n",
    "\n",
    "def model_fn(layer_one_neurons=32, layer_two_neurons=32,layer_three_neurons=32, \n",
    "             layer_one_regularizer=\"l1(0.001)\", layer_two_regularizer=\"l1(0.001)\", layer_three_regularizer=\"l1(0.001)\"):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a convolutional layer.\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(1196, 780, 3)) )\n",
    "    \n",
    "    # add a pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Add a convolutional layer.\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(1196, 780, 3)) )\n",
    "    \n",
    "    # add a pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(layer_one_neurons, activation='relu', kernel_regularizer=layer_one_regularizer))\n",
    " \n",
    "    \n",
    "    model.add(Dense(layer_two_neurons, activation='relu', kernel_regularizer=layer_two_regularizer))\n",
    "    \n",
    "    model.add(Dense(layer_three_neurons, activation='relu', kernel_regularizer=layer_three_regularizer))\n",
    "    \n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "nn = KerasRegressor(build_fn=model_fn, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The params grid:\n",
    "nm_params = {\n",
    "            'layer_one_neurons':[8, 16, 32, 64, 128],\n",
    "             'layer_two_neurons':[8, 16, 32, 64, 128],\n",
    "            'layer_three_neurons':[8, 16, 32, 64, 128],\n",
    "             'layer_one_regularizer':[\"l1(0.001)\", \"l1(0.01)\", \"l1(0.0001)\"],\n",
    "            'layer_two_regularizer':[\"l1(0.001)\", \"l1(0.01)\", \"l1(0.0001)\"],\n",
    "            'layer_three_regularizer':[\"l1(0.001)\", \"l1(0.01)\", \"l1(0.0001)\"],\n",
    "            'epochs':[10, 15, 20]\n",
    "            }\n",
    "\n",
    "gs = GridSearchCV(estimator=nn, param_grid=nm_params, cv=2) # Use cv=3 just for the sake of time! \n",
    "gs.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another model with Ridge regularizer\n",
    "\n",
    "def model_fn(layer_one_neurons=32, layer_two_neurons=32,layer_three_neurons=32, \n",
    "             layer_one_regularizer=\"l2(0.001)\", layer_two_regularizer=\"l2(0.001)\", layer_three_regularizer=\"l2(0.001)\"):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a convolutional layer.\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(1196, 780, 3)) )\n",
    "    \n",
    "    # add a pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Add a convolutional layer.\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(1196, 780, 3)) )\n",
    "    \n",
    "    # add a pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(layer_one_neurons, activation='relu', kernel_regularizer=layer_one_regularizer))\n",
    " \n",
    "    \n",
    "    model.add(Dense(layer_two_neurons, activation='relu', kernel_regularizer=layer_two_regularizer))\n",
    "    \n",
    "    model.add(Dense(layer_three_neurons, activation='relu', kernel_regularizer=layer_three_regularizer))\n",
    "    \n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "nn = KerasRegressor(build_fn=model_fn, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The params grid:\n",
    "nm_params = {\n",
    "            'layer_one_neurons':[8, 16, 32, 64, 128],\n",
    "             'layer_two_neurons':[8, 16, 32, 64, 128],\n",
    "            'layer_three_neurons':[8, 16, 32, 64, 128],\n",
    "             'layer_one_regularizer':[\"l2(0.001)\", \"l2(0.01)\", \"l2(0.0001)\"],\n",
    "            'layer_two_regularizer':[\"l2(0.001)\", \"l2(0.01)\", \"l2(0.0001)\"],\n",
    "            'layer_three_regularizer':[\"l2(0.001)\", \"l2(0.01)\", \"l2(0.0001)\"],\n",
    "            'epochs':[10, 15, 20]\n",
    "            }\n",
    "\n",
    "gs = GridSearchCV(estimator=nn, param_grid=nm_params, cv=2) # Use cv=3 just for the sake of time! \n",
    "gs.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
